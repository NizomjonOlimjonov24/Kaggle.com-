{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10017073,"sourceType":"datasetVersion","datasetId":6079696}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For CUDA 10.2\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu102","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:20.533394Z","iopub.execute_input":"2024-11-26T20:11:20.534089Z","iopub.status.idle":"2024-11-26T20:11:28.539087Z","shell.execute_reply.started":"2024-11-26T20:11:20.534054Z","shell.execute_reply":"2024-11-26T20:11:28.53796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets, models\nfrom torchvision.transforms import functional as FT\nfrom torchvision import transforms as T\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, sampler, random_split, Dataset\nimport copy\nimport math\nfrom PIL import Image\nimport cv2\nimport albumentations as A\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom collections import defaultdict, deque\nimport datetime\nimport time\nfrom tqdm import tqdm # progress bar\nfrom torchvision.utils import draw_bounding_boxes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:28.542152Z","iopub.execute_input":"2024-11-26T20:11:28.543084Z","iopub.status.idle":"2024-11-26T20:11:33.624696Z","shell.execute_reply.started":"2024-11-26T20:11:28.543032Z","shell.execute_reply":"2024-11-26T20:11:33.623793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.__version__)\nprint(torch.version.cuda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:33.625877Z","iopub.execute_input":"2024-11-26T20:11:33.626416Z","iopub.status.idle":"2024-11-26T20:11:33.631153Z","shell.execute_reply.started":"2024-11-26T20:11:33.626375Z","shell.execute_reply":"2024-11-26T20:11:33.630222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pycocotools\nfrom pycocotools.coco import COCO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:33.632797Z","iopub.execute_input":"2024-11-26T20:11:33.63306Z","iopub.status.idle":"2024-11-26T20:11:41.677953Z","shell.execute_reply.started":"2024-11-26T20:11:33.633036Z","shell.execute_reply":"2024-11-26T20:11:41.677038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from albumentations.pytorch import ToTensorV2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:41.679475Z","iopub.execute_input":"2024-11-26T20:11:41.679771Z","iopub.status.idle":"2024-11-26T20:11:41.686367Z","shell.execute_reply.started":"2024-11-26T20:11:41.679741Z","shell.execute_reply":"2024-11-26T20:11:41.68569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_transforms(train=False):\n    if train:\n        transform = A.Compose([\n            A.Resize(600, 600), # our input size can be 600px\n            A.HorizontalFlip(p=0.3),\n            A.VerticalFlip(p=0.3),\n            A.RandomBrightnessContrast(p=0.1),\n            A.ColorJitter(p=0.1),\n            ToTensorV2()\n        ], bbox_params=A.BboxParams(format='coco'))\n    else:\n        transform = A.Compose([\n            A.Resize(600, 600), # our input size can be 600px\n            ToTensorV2()\n        ], bbox_params=A.BboxParams(format='coco'))\n    return transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:41.687325Z","iopub.execute_input":"2024-11-26T20:11:41.687577Z","iopub.status.idle":"2024-11-26T20:11:41.699136Z","shell.execute_reply.started":"2024-11-26T20:11:41.687553Z","shell.execute_reply":"2024-11-26T20:11:41.698302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DroneDetection(datasets.VisionDataset):\n    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n        # the 3 transform parameters are reuqired for datasets.VisionDataset\n        super().__init__(root, transforms, transform, target_transform)\n        self.split = split #train, valid, test\n        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) # annotatiosn stored here\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n    \n    def _load_image(self, id: int):\n        path = self.coco.loadImgs(id)[0]['file_name']\n        image = cv2.imread(os.path.join(self.root, self.split, path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n    def _load_target(self, id):\n        return self.coco.loadAnns(self.coco.getAnnIds(id))\n    \n    def __getitem__(self, index):\n        id = self.ids[index]\n        image = self._load_image(id)\n        target = self._load_target(id)\n        target = copy.deepcopy(self._load_target(id))\n        \n        boxes = [t['bbox'] + [t['category_id']] for t in target] # required annotation format for albumentations\n        if self.transforms is not None:\n            transformed = self.transforms(image=image, bboxes=boxes)\n        \n        image = transformed['image']\n        boxes = transformed['bboxes']\n        \n        new_boxes = [] # convert from xywh to xyxy\n        for box in boxes:\n            xmin = box[0]\n            xmax = xmin + box[2]\n            ymin = box[1]\n            ymax = ymin + box[3]\n            new_boxes.append([xmin, ymin, xmax, ymax])\n        \n        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n        \n        targ = {} # here is our transformed target\n        targ['boxes'] = boxes\n        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n        return image.div(255), targ # scale images\n    def __len__(self):\n        return len(self.ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:41.700158Z","iopub.execute_input":"2024-11-26T20:11:41.700408Z","iopub.status.idle":"2024-11-26T20:11:41.712714Z","shell.execute_reply.started":"2024-11-26T20:11:41.700384Z","shell.execute_reply":"2024-11-26T20:11:41.711823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/drone-detection/coco json drone detection\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:41.713617Z","iopub.execute_input":"2024-11-26T20:11:41.713884Z","iopub.status.idle":"2024-11-26T20:11:41.727074Z","shell.execute_reply.started":"2024-11-26T20:11:41.713858Z","shell.execute_reply":"2024-11-26T20:11:41.726269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"coco = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\ncategories = coco.cats\nn_classes = len(categories.keys())\ncategories","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:41.727859Z","iopub.execute_input":"2024-11-26T20:11:41.728102Z","iopub.status.idle":"2024-11-26T20:11:41.82276Z","shell.execute_reply.started":"2024-11-26T20:11:41.728078Z","shell.execute_reply":"2024-11-26T20:11:41.821935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = [i[1]['name'] for i in categories.items()]\nclasses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:41.825435Z","iopub.execute_input":"2024-11-26T20:11:41.825706Z","iopub.status.idle":"2024-11-26T20:11:41.831353Z","shell.execute_reply.started":"2024-11-26T20:11:41.82568Z","shell.execute_reply":"2024-11-26T20:11:41.830539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = DroneDetection(root=dataset_path, transforms=get_transforms(True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:41.832348Z","iopub.execute_input":"2024-11-26T20:11:41.832599Z","iopub.status.idle":"2024-11-26T20:11:42.131244Z","shell.execute_reply.started":"2024-11-26T20:11:41.832574Z","shell.execute_reply":"2024-11-26T20:11:42.13039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = train_dataset[2]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(\n    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n).permute(1, 2, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:42.132521Z","iopub.execute_input":"2024-11-26T20:11:42.132867Z","iopub.status.idle":"2024-11-26T20:11:42.651655Z","shell.execute_reply.started":"2024-11-26T20:11:42.132829Z","shell.execute_reply":"2024-11-26T20:11:42.650764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:11:42.652734Z","iopub.execute_input":"2024-11-26T20:11:42.653007Z","iopub.status.idle":"2024-11-26T20:11:42.658352Z","shell.execute_reply.started":"2024-11-26T20:11:42.652964Z","shell.execute_reply":"2024-11-26T20:11:42.657555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\nin_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:34:55.479704Z","iopub.execute_input":"2024-11-26T20:34:55.480618Z","iopub.status.idle":"2024-11-26T20:34:55.796707Z","shell.execute_reply.started":"2024-11-26T20:34:55.480569Z","shell.execute_reply":"2024-11-26T20:34:55.795762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:34:57.396468Z","iopub.execute_input":"2024-11-26T20:34:57.397209Z","iopub.status.idle":"2024-11-26T20:34:57.400917Z","shell.execute_reply.started":"2024-11-26T20:34:57.397173Z","shell.execute_reply":"2024-11-26T20:34:57.400122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:35:02.85392Z","iopub.execute_input":"2024-11-26T20:35:02.854274Z","iopub.status.idle":"2024-11-26T20:35:02.859039Z","shell.execute_reply.started":"2024-11-26T20:35:02.854242Z","shell.execute_reply":"2024-11-26T20:35:02.858052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images,targets = next(iter(train_loader))\nimages = list(image for image in images)\ntargets = [{k:v for k, v in t.items()} for t in targets]\noutput = model(images, targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:35:03.753838Z","iopub.execute_input":"2024-11-26T20:35:03.754652Z","iopub.status.idle":"2024-11-26T20:35:05.847869Z","shell.execute_reply.started":"2024-11-26T20:35:03.754615Z","shell.execute_reply":"2024-11-26T20:35:05.846764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:35:05.849811Z","iopub.execute_input":"2024-11-26T20:35:05.850132Z","iopub.status.idle":"2024-11-26T20:35:05.854565Z","shell.execute_reply.started":"2024-11-26T20:35:05.850101Z","shell.execute_reply":"2024-11-26T20:35:05.853685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:35:06.034093Z","iopub.execute_input":"2024-11-26T20:35:06.034772Z","iopub.status.idle":"2024-11-26T20:35:06.06709Z","shell.execute_reply.started":"2024-11-26T20:35:06.034735Z","shell.execute_reply":"2024-11-26T20:35:06.066221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:35:06.999903Z","iopub.execute_input":"2024-11-26T20:35:07.000271Z","iopub.status.idle":"2024-11-26T20:35:07.006107Z","shell.execute_reply.started":"2024-11-26T20:35:07.000238Z","shell.execute_reply":"2024-11-26T20:35:07.00514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n\nmetrics={'epochs':[],'loss':[]}\ndef train_one_epoch(model, optimizer, loader, device, epoch):\n    model.to(device)\n    model.train()\n    all_losses = []\n    all_losses_dict = []\n    \n    for images, targets in tqdm(loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n        loss_value = losses.item()\n        \n        all_losses.append(loss_value)\n        all_losses_dict.append(loss_dict_append)\n        \n        if not math.isfinite(loss_value):\n            print(f\"Loss is {loss_value}, stopping trainig\")\n            print(loss_dict)\n            sys.exit(1)\n        \n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n\n        \n    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n        all_losses_dict['loss_classifier'].mean(),\n        all_losses_dict['loss_box_reg'].mean(),\n        all_losses_dict['loss_rpn_box_reg'].mean(),\n        all_losses_dict['loss_objectness'].mean()\n    ))\n\n    metrics['loss'].append(np.mean(all_losses))\n    metrics['epochs'].append(epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:35:07.75618Z","iopub.execute_input":"2024-11-26T20:35:07.75653Z","iopub.status.idle":"2024-11-26T20:35:07.765502Z","shell.execute_reply.started":"2024-11-26T20:35:07.756499Z","shell.execute_reply":"2024-11-26T20:35:07.764532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs=30\n\nfor epoch in range(num_epochs):\n    train_one_epoch(model, optimizer, train_loader, device, epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:35:08.629424Z","iopub.execute_input":"2024-11-26T20:35:08.63028Z","iopub.status.idle":"2024-11-26T20:40:58.537409Z","shell.execute_reply.started":"2024-11-26T20:35:08.630245Z","shell.execute_reply":"2024-11-26T20:40:58.535802Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T20:41:02.975478Z","iopub.execute_input":"2024-11-26T20:41:02.975803Z","iopub.status.idle":"2024-11-26T20:41:02.992233Z","shell.execute_reply.started":"2024-11-26T20:41:02.975775Z","shell.execute_reply":"2024-11-26T20:41:02.991211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iou_score(loader):\n    scores=[]\n    n=0\n    for images, targets in tqdm(loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n        n+=len(images)\n        try:\n            x_target_min,y_target_min,x_target_max,y_target_max=targets[0]['boxes'].tolist()[0]\n        except:\n            continue\n        \n        prediction = model(images)\n        pred = prediction[0]\n\n        try:\n            x_min,y_min,x_max,y_max=pred['boxes'][pred['scores'] > 0.8].tolist()[0]\n        except:\n            continue\n\n        inter_x_min = max(x_min, x_target_min)\n        inter_y_min = max(y_min, y_target_min)\n        inter_x_max = min(x_max, x_target_max)\n        inter_y_max = min(y_max, y_target_max)\n\n        inter_width = max(0, inter_x_max - inter_x_min + 1)\n        inter_height = max(0, inter_y_max - inter_y_min + 1)\n        intersection_area = inter_width * inter_height\n\n        box1_area = (x_max - x_min + 1) * (y_max - y_min + 1)\n        box2_area = (x_target_max - x_target_min + 1) * (y_target_max - y_target_min + 1)\n\n        union_area = box1_area + box2_area - intersection_area\n\n        if union_area == 0:\n            score+=0\n\n        scores.append(intersection_area / union_area)\n\n    return sum(scores)/len(scores)\nprint(iou_score(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T21:20:36.80526Z","iopub.execute_input":"2024-11-26T21:20:36.806106Z","iopub.status.idle":"2024-11-26T21:23:05.814214Z","shell.execute_reply.started":"2024-11-26T21:20:36.806068Z","shell.execute_reply":"2024-11-26T21:23:05.813189Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = DroneDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:50:19.025319Z","iopub.execute_input":"2024-11-26T15:50:19.025669Z","iopub.status.idle":"2024-11-26T15:50:19.040075Z","shell.execute_reply.started":"2024-11-26T15:50:19.025639Z","shell.execute_reply":"2024-11-26T15:50:19.039243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, _ = test_dataset[100]\nimg_int = torch.tensor(img*255, dtype=torch.uint8)\nwith torch.no_grad():\n    prediction = model([img.to(device)])\n    pred = prediction[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:50:20.366908Z","iopub.execute_input":"2024-11-26T15:50:20.367276Z","iopub.status.idle":"2024-11-26T15:50:20.406205Z","shell.execute_reply.started":"2024-11-26T15:50:20.367244Z","shell.execute_reply":"2024-11-26T15:50:20.405359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(14, 10))\nplt.imshow(draw_bounding_boxes(img_int,\n    pred['boxes'][pred['scores'] > 0.8],\n    [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n).permute(1, 2, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:50:21.619791Z","iopub.execute_input":"2024-11-26T15:50:21.620162Z","iopub.status.idle":"2024-11-26T15:50:22.453356Z","shell.execute_reply.started":"2024-11-26T15:50:21.620129Z","shell.execute_reply":"2024-11-26T15:50:22.452391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(metrics['epochs'], metrics['loss'], marker='o', label='Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss vs. Epochs')\nplt.grid()\nplt.legend()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:50:30.548921Z","iopub.execute_input":"2024-11-26T15:50:30.549314Z","iopub.status.idle":"2024-11-26T15:50:30.857899Z","shell.execute_reply.started":"2024-11-26T15:50:30.549281Z","shell.execute_reply":"2024-11-26T15:50:30.856949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}